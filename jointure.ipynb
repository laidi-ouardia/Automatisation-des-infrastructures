{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jointure.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1h6t-Ps1jJu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import *\n",
        "\n",
        "spark =  SparkSession.builder.getOrCreate()\n",
        "\n",
        "data1 = spark.read.csv('reponse.csv', header = True, sep = \";\" ) \n",
        "data2 = spark.read.csv('table2.csv',  header = True , sep = \";\" ) \n",
        "data2.printSchema()\n",
        "data2.show()\n",
        "\n",
        "table_res = data1.join(data2, data1.libelle == data2.libelle , \"outer\")\n",
        "table_res.show()\n",
        "table_res.drop\n",
        "\n",
        "table_res = table_res.drop('iu_ac')\n",
        "table_res = table_res.drop('t_1h')\n",
        "table_res = table_res.drop('libelle')\n",
        "table_res = table_res.drop('etat_trafic')\n",
        "\n",
        "table_res.write.csv(\"table_res.csv\", sep=';')\n",
        "\n",
        "table_res=table_res.assign(count=0)\n",
        "grouped_count = table_res[[\"arrondissement\", \"etat_trafic_y\",\"count\"]] \\\n",
        "        .groupby(by=[\"arrondissement\", \"etat_trafic_y\"]) \\\n",
        "        .count()  \\\n",
        "        .reset_index()\n",
        "\n",
        "subprocess.call(['hadoop fs -copyFromLocal table_resultante.csv hdfs:///data/groupe16'],shell=True)"
      ]
    }
  ]
}